## rules to process nuclear SNV library

### input, output and shell paths are all relative to the project directory ###

configfile: "config.yml"

import glob

rule all:
    input:
        expand("raw_data/{patient}/selected_variants.csv",
               patient = config["patient_ids"]),
        expand("data/{patient}/mutation_counts/reads_cell.txt",
               patient = config["patient_ids"]),
        expand("data/{patient}/align_reads/gene_tagged_aligned.bam",
               patient = config["patient_ids"]),
        expand("data/{patient}/mutation_counts/alignment_stats.csv",
               patient = config["patient_ids"]),
        expand("data/{patient}/mutation_counts/count_table/umi_collapsed.bam",
               patient = config["patient_ids"]),
        expand("data/{patient}/mutation_counts/count_table/{patient}_count_table.rds",
               patient = config["patient_ids"]),
        expand("results/summary_reports/{patient}/{patient}_report.html",
               patient = config["patient_ids"])

# this is necessary to avoid ambiguous rules
wildcard_constraints:
    patient='|'.join(config["patient_ids"])

# workflow rules -----------------------------------------------------------------------------------

# convert fastq input files into one unmapped bam file
rule fastq_to_bam:
  input:
    fastq1 = lambda wildcards: glob.glob('raw_data/{patient}/*R1*'.format(patient=wildcards.patient)),
    fastq2 = lambda wildcards: glob.glob('raw_data/{patient}/*R2*'.format(patient=wildcards.patient))
  output:
    temp("data/{patient}/align_reads/unmapped.bam")
  log:
    "data/{patient}/align_reads/logs/fastq_to_bam.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "picard FastqToSam "
    "FASTQ={input.fastq1} "
    "FASTQ2={input.fastq2} "
    "OUTPUT={output} "
    "SAMPLE_NAME={wildcards.patient} "
    "SORT_ORDER=queryname "
    "TMP_DIR=./tmp "
    "2> {log}"

# tag genome reads with CELL barcodes
rule tag_cell_barcodes:
  input:
    "data/{patient}/align_reads/unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/cell_tagged_unmapped.bam"),
    summary = "data/{patient}/align_reads/cell_tags_summary.txt"
  params:
    base_range = config["bc_structure"]["10x_v3"][0],
    base_qual = config["tag_cell_barcodes"]["base_quality"],
    bases_below_qual = config["tag_cell_barcodes"]["num_bases_below_quality"]
  log:
    "data/{patient}/align_reads/logs/tag_cell_barcodes.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "TagBamWithReadSequenceExtended "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "SUMMARY={output.summary} "
    "BASE_QUALITY={params.base_qual} "
    "NUM_BASES_BELOW_QUALITY={params.bases_below_qual} "
    "BASE_RANGE={params.base_range} "
    "BARCODED_READ=1 "
    "DISCARD_READ=false "
    "TAG_NAME=CB "
    "2> {log}"

# tag genome reads with MOLECULE barcodes
rule tag_molecule_barcodes:
  input:
    "data/{patient}/align_reads/cell_tagged_unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/mol_tagged_unmapped.bam"),
    summary = "data/{patient}/align_reads/mol_tags_summary.txt"
  params:
    base_range = config["bc_structure"]["10x_v3"][1],
    base_qual = config["tag_cell_barcodes"]["base_quality"],
    bases_below_qual = config["tag_cell_barcodes"]["num_bases_below_quality"]
  log:
    "data/{patient}/align_reads/logs/tag_molecule_barcodes.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "TagBamWithReadSequenceExtended "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "SUMMARY={output.summary} "
    "BASE_QUALITY={params.base_qual} "
    "NUM_BASES_BELOW_QUALITY={params.bases_below_qual} "
    "BASE_RANGE={params.base_range} "
    "BARCODED_READ=1 "
    "DISCARD_READ=true "
    "TAG_NAME=UB "
    "2> {log}"

# filter reads marked as 'rejected' by TagBamWithReadSequenceExtended
rule filter_bam:
  input:
    "data/{patient}/align_reads/mol_tagged_unmapped.bam"
  output:
    temp("data/{patient}/align_reads/filt_unmapped.bam")
  log:
    "data/{patient}/align_reads/logs/filter_bam.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "FilterBam "
    "INPUT={input} "
    "OUTPUT={output} "
    "TAG_REJECT=XQ "
    "2> {log}"

# trim SMART adapter sequences from 5'
rule trim_starting_sequence:
  input:
    "data/{patient}/align_reads/filt_unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/adapter_trimmed_unmapped.bam"),
    summary = "data/{patient}/align_reads/adapter_trimming_report.txt"
  params:
    adapter_sequence = config["trim_starting_sequence"]["adapter_sequence"],
    mismatches = config["trim_starting_sequence"]["mismatches"],
    num_bases = config["trim_starting_sequence"]["num_bases"]
  log:
    "data/{patient}/align_reads/logs/trim_starting_sequence.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "TrimStartingSequence "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "OUTPUT_SUMMARY={output.summary} "
    "SEQUENCE={params.adapter_sequence} "
    "MISMATCHES={params.mismatches} "
    "NUM_BASES={params.num_bases} "
    "2> {log}"

# trim polyA sequences from 3'
rule trim_polyA:
  input:
    "data/{patient}/align_reads/adapter_trimmed_unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/polyA_trimmed_unmapped.bam"),
    summary = "data/{patient}/align_reads/polyA_trimming_report.txt"
  params:
    mismatches = config["trim_polyA"]["mismatches"],
    num_bases = config["trim_polyA"]["num_bases"]
  log:
    "data/{patient}/align_reads/logs/trim_polyA.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "PolyATrimmer "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "OUTPUT_SUMMARY={output.summary} "
    "MISMATCHES={params.mismatches} "
    "NUM_BASES={params.num_bases} "
    "2> {log}"

# convert to fastq for STAR read aligner
rule sam_to_fastq:
  input:
    "data/{patient}/align_reads/polyA_trimmed_unmapped.bam"
  output:
    temp("data/{patient}/align_reads/polyA_trimmed_unmapped.fastq.gz")
  log:
    "data/{patient}/align_reads/logs/sam_to_fastq.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "picard SamToFastq "
    "INPUT={input} "
    "FASTQ={output} "
    "TMP_DIR=./tmp "
    "2> {log}"

#align reads using STAR
rule star_align:
  input:
    fastq = "data/{patient}/align_reads/polyA_trimmed_unmapped.fastq.gz"
  output:
    bam = temp("data/{patient}/align_reads/star.Aligned.out.bam"),
    final_log = "data/{patient}/align_reads/star.Log.final.out"
  params:
    outprefix = "data/{patient}/align_reads/star.",
    genome_dir = config["genome_references"]["genome_dir"],
    threads = config["star_align"]["threads_comp_bam"]
  conda:
    "envs/nuclear_library.yml"
  shell:
    "STAR "
    "--runThreadN {params.threads} "
    "--genomeDir {params.genome_dir} "
    "--readFilesIn {input.fastq} "
    "--outFileNamePrefix {params.outprefix} "
    "--readFilesCommand zcat "
    "--limitOutSJcollapsed 5000000 "
    "--outSAMtype BAM Unsorted ; "
    # move STAR "progress" logs into log directory
    "mv data/{wildcards.patient}/align_reads/star.Log.progress.out "
    "data/{wildcards.patient}/align_reads/logs ; "
    "mv data/{wildcards.patient}/align_reads/star.Log.out "
    "data/{wildcards.patient}/align_reads/logs ; "
    "mv data/{wildcards.patient}/align_reads/star.SJ.out.tab "
    "data/{wildcards.patient}/align_reads/logs "

# sort aligned reads
rule sort_aligned:
  input:
    "data/{patient}/align_reads/star.Aligned.out.bam"
  output:
    temp("data/{patient}/align_reads/star.Aligned.sorted.bam")
  log:
    "data/{patient}/align_reads/logs/sort_aligned.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "picard SortSam "
    "INPUT={input} "
    "OUTPUT={output} "
    "SORT_ORDER=queryname "
    "TMP_DIR=./tmp "
    "2> {log}"


# merge aligned and unaligned reads to add tags to aligned reads. this also removes secondary
# alignments!
rule merge_bam:
  input:
    aligned = "data/{patient}/align_reads/star.Aligned.sorted.bam",
    unaligned = "data/{patient}/align_reads/polyA_trimmed_unmapped.bam",
    reference = config["genome_references"]["fasta"],
    dict = config["genome_references"]["dict"]
  output:
    temp("data/{patient}/align_reads/merged_aligned.bam")
  log:
    "data/{patient}/align_reads/logs/merge_bam.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "picard MergeBamAlignment "
    "ALIGNED_BAM={input.aligned} "
    "UNMAPPED_BAM={input.unaligned} "
    "REFERENCE_SEQUENCE={input.reference} "
    "OUTPUT={output} "
    "INCLUDE_SECONDARY_ALIGNMENTS=false "
    "PAIRED_RUN=false "
    "TMP_DIR=./tmp "
    "2> {log}"


# index BAM file in order to change barcode barcode format
rule index_bam:
    input:
        "data/{patient}/align_reads/merged_aligned.bam"
    output:
        temp("data/{patient}/align_reads/merged_aligned.bam.bai")
    log:
        "data/{patient}/align_reads/logs/index_bam.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "samtools index -b {input} "
        "2> {log}"


# change cell barcodes to cellranger format (adding -1) and filter reads not coming
# from valid cell barcodes (list output by cellranger)
rule reformat_cellbarcode:
    input:
        bam = "data/{patient}/align_reads/merged_aligned.bam",
        bai = "data/{patient}/align_reads/merged_aligned.bam.bai",
        barcodes = "raw_data/{patient}/barcodes.tsv"
    output:
        bam = temp("data/{patient}/align_reads/merged_aligned_tagged.bam")
    log:
        "data/{patient}/align_reads/logs/reformat_barcodes.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "python scripts/reformat_barcode.py "
        "-i {input.bam} "
        "-b {input.barcodes} "
        "-o {output.bam} "
        "2> {log}"


# tag reads with gene exons
rule tag_with_gene_exon:
  input:
    bam = "data/{patient}/align_reads/merged_aligned_tagged.bam",
    annot = config["genome_references"]["refFlat"]
  output:
    bam = "data/{patient}/align_reads/gene_tagged_aligned.bam"
  log:
    "data/{patient}/align_reads/logs/tag_with_gene_exon.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "TagReadWithGeneFunction "
    "INPUT={input.bam} "
    "OUTPUT={output.bam} "
    "ANNOTATIONS_FILE={input.annot} "
    "CREATE_INDEX=true "
    "2> {log}"


# calculate reads per cell barcode
rule reads_per_cell:
  input:
    "data/{patient}/align_reads/gene_tagged_aligned.bam"
  output:
    "data/{patient}/align_reads/reads_per_cell_barcode.txt"
  params:
    read_quality = config["reads_per_cell"]["read_quality"]
  log:
    "data/{patient}/align_reads/logs/reads_per_cell.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "BamTagHistogram "
    "INPUT={input} "
    "OUTPUT={output} "
    "TAG=CB "
    "READ_MQ={params.read_quality} "
    "2> {log}"

# get total reads per cell
rule get_reads_cell:
    input:
        bam = "data/{patient}/align_reads/gene_tagged_aligned.bam"
    output:
        reads_cell = "data/{patient}/mutation_counts/reads_cell.txt"
    log:
        "data/{patient}/mutation_counts/logs/get_reads_cell.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "samtools view {input.bam} | awk '{{print $12}}' | sed 's/CB\:Z\://' | sort | uniq -c | "
        "awk '{{if($1 > 100) {{print}} }}' > {output.reads_cell}"


# generate a table with the proportion of reads aligning to genes of interest, exons, mitochondria, and unmapped.
rule alignment_summary_stats:
  input:
    bam = "data/{patient}/align_reads/gene_tagged_aligned.bam"
  output:
    "data/{patient}/mutation_counts/alignment_stats.csv"
  params:
    gtf_genome = config["genome_references"]["gtf"],
    target_genes = "raw_data/{patient}/selected_variants.csv"
  log:
    "data/{patient}/mutation_counts/logs/alignment_positions.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "python scripts/align_stats_mutations.py "
    "--bam {input.bam} "
    "--gtf_genome {params.gtf_genome} "
    "--output {output} "
    "--genes {params.target_genes} "
    "2> {log}"

# make bed file with the mutated sites
rule make_bed:
  input:
    genes = "raw_data/{patient}/selected_variants.csv",
    bam = "data/{patient}/align_reads/gene_tagged_aligned.bam"
  output:
    "data/{patient}/mutation_counts/mutated_sites.bed"
  log:
    "data/{patient}/mutation_counts/logs/make_bed.log"
  conda:
    "envs/r_processing.yml"
  shell:
    "Rscript scripts/make_bed.R "
    "-i {input.genes} "
    "-b {input.bam} "
    "-o {output} "
    "2> {log}"


# filter reads not overlapping with the mutated sites
rule filter_bam_mutations:
  input:
    bam = "data/{patient}/align_reads/gene_tagged_aligned.bam",
    bed = "data/{patient}/mutation_counts/mutated_sites.bed"
  output:
    temp("data/{patient}/mutation_counts/mutation_filtered.bam")
  log:
    "data/{patient}/mutation_counts/logs/filter_bam_mutations.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "samtools view -b -h -L {input.bed} {input.bam} > {output} "
    "2> {log}"

#rule to get present barcodes (it will avoid creating empty single-cell BAM files)
rule get_barcodes:
    input:
        "data/{patient}/mutation_counts/mutation_filtered.bam"
    output:
        temp("data/{patient}/align_reads/present_barcodes.txt")
    log:
        "data/{patient}/align_reads/logs/get_present_barcodes.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "samtools view {input} | awk '{{print $12}}' | sed 's/CB\:Z\://' | sort | uniq > {output} "
        "2> {log}"

# split barcoded BAM file into single-cell files using CB barcode
# since we don't know in advance the number of files I defined as checkpoint so that the DAG gets reevaluated here
# and in the following rules
checkpoint split_bam:
  input:
    bam = "data/{patient}/mutation_counts/mutation_filtered.bam",
    barcodes = "raw_data/{patient}/barcodes.tsv",
    present_barcodes = "data/{patient}/align_reads/present_barcodes.txt"
  output:
    directory("data/{patient}/mutation_counts/count_table/temp_bams/split_bams")
  params:
    cores = config["read_deduplication"]["cores_split_files"],
    max_files = config["read_deduplication"]["max_open_files"]
  log:
    "data/{patient}/mutation_counts/logs/split_bam.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "python scripts/split_bam_single_files.py "
    "-i {input.bam} "
    "-o {output} "
    "-b {input.barcodes} "
    "-p {input.present_barcodes} "
    "-c {params.cores} "
    "-m {params.max_files} "
    "2> {log}"

# rule to make consensus reads
rule consensus_read:
  input:
    "data/{patient}/mutation_counts/count_table/temp_bams/split_bams/{barcode}.bam"
  output:
    "data/{patient}/mutation_counts/count_table/temp_bams/processed/{barcode}.bam"
  log:
    "data/{patient}/mutation_counts/logs/make_consensus/{barcode}.log"
  params:
    errors = config["read_deduplication"]["umi_errors"],
    min_reads = config["read_deduplication"]["min_reads_umi"]
  conda:
    "envs/nuclear_library.yml"
  shell:
    "python scripts/make_consensus_read.py "
    "-i {input} "
    "-o {output} "
    "-e {params.errors} "
    "-r {params.min_reads} "
    "2> {log}"


# make function to aggregate input after checkpoint
def aggregate_bams(wildcards):

    checkpoint_output = checkpoints.split_bam.get(patient = wildcards.patient).output[0]

    return expand("data/{patient}/mutation_counts/count_table/temp_bams/processed/{barcode}.bam",
                  patient = wildcards.patient,
                  barcode = glob_wildcards(os.path.join(checkpoint_output, "{barcode}.bam")).barcode)

# merge single-cell BAM files with unique read names to a single file
# it takes ~30' to run, I think because gathering all input files takes a while in Snakemake
rule combine_bams:
  input:
    aggregate_bams
  output:
    temp("data/{patient}/mutation_counts/count_table/unmapped_tagged.bam")
  params:
    indir = "data/{patient}/mutation_counts/count_table/temp_bams/processed/",
    outdir = "data/{patient}/mutation_counts/count_table/",
    max_files = config["read_deduplication"]["max_open_files"],
    cores = config["read_deduplication"]["cores_merge_bam"]
  log:
    "data/{patient}/mutation_counts/logs/combine_single_bams.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "python scripts/merge_single_bams.py "
    "-i {params.indir} "
    "-o {output} "
    "-d {params.outdir} "
    "-m {params.max_files} "
    "-c {params.cores} "
    "2> {log}"

# sort bam by queryname
rule sort_bam_query:
 input:
   "data/{patient}/mutation_counts/count_table/unmapped_tagged.bam"
 output:
   temp("data/{patient}/mutation_counts/count_table/unmapped_sorted.bam")
 log:
   "data/{patient}/mutation_counts/logs/sort_merged_bam.log"
 conda:
   "envs/nuclear_library.yml"
 shell:
   "picard SortSam "
   "INPUT={input} "
   "OUTPUT={output} "
   "SORT_ORDER=queryname "
   "TMP_DIR=./tmp "
   "2> {log}"


# transform bam to fastqs for alignment
rule dedupl_bam_fastq:
  input:
    "data/{patient}/mutation_counts/count_table/unmapped_sorted.bam"
  output:
    temp("data/{patient}/mutation_counts/count_table/consensus.fastq.gz")
  log:
    "data/{patient}/mutation_counts/logs/bam_to_fastq.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "picard SamToFastq "
    "INPUT={input} "
    "FASTQ={output} "
    "TMP_DIR=./tmp "
    "2> {log}"

# align deduplicated fastq using STAR (takes ~2-3min file)
rule align_deduplicated:
  input:
    "data/{patient}/mutation_counts/count_table/consensus.fastq.gz"
  output:
    bam = temp("data/{patient}/mutation_counts/count_table/mapped.bam")
  params:
    threads = config["read_deduplication"]["star_threads"],
    ref_genome = config["genome_references"]["genome_dir"],
    prefix = "data/{patient}/mutation_counts/count_table/star/",
    out_bam = "data/{patient}/mutation_counts/count_table/star/*.bam"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "mkdir -p {params.prefix} ; "
    "STAR "
    "--runThreadN {params.threads} "
    "--genomeDir {params.ref_genome} "
    "--readFilesIn {input} "
    "--outFileNamePrefix {params.prefix} "
    "--readFilesCommand zcat "
    "--limitOutSJcollapsed 5000000 "
    "--outSAMtype BAM Unsorted ; "
    # move bam file from star_logs folder to count_bams folder
    "mv {params.out_bam} {output.bam}"

# merge aligned and unaligned reads to add tags to aligned reads. this also removes secondary
# alignments!
rule merge_bams:
  input:
    aligned = "data/{patient}/mutation_counts/count_table/mapped.bam",
    unaligned = "data/{patient}/mutation_counts/count_table/unmapped_sorted.bam",
    reference = config["genome_references"]["fasta"],
  output:
    "data/{patient}/mutation_counts/count_table/umi_collapsed.bam"
  log:
    "data/{patient}/mutation_counts/logs/merge_bam.log"
  conda:
    "envs/nuclear_library.yml"
  shell:
    "picard MergeBamAlignment "
    "ALIGNED_BAM={input.aligned} "
    "UNMAPPED_BAM={input.unaligned} "
    "REFERENCE_SEQUENCE={input.reference} "
    "OUTPUT={output} "
    "INCLUDE_SECONDARY_ALIGNMENTS=false "
    "PAIRED_RUN=false "
    "TMP_DIR=./tmp "
    "2> {log}"

# index deduplicated sorted bam (do this in the get_mutation_counts script)
rule index_deduplicated_bam:
    input:
        "data/{patient}/mutation_counts/count_table/umi_collapsed.bam"
    output:
        "data/{patient}/mutation_counts/count_table/umi_collapsed.bam.bai"
    log:
        "data/{patient}/mutation_counts/logs/index_merged_bam.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "samtools index -b {input} "
        "2> {log}"

# get count tables with ref and alt counts, coverage and missmatch ratio per cell and site
rule make_count_table:
    input:
        bam = "data/{patient}/mutation_counts/count_table/umi_collapsed.bam",
        bai =  "data/{patient}/mutation_counts/count_table/umi_collapsed.bam.bai",
        variants = "raw_data/{patient}/selected_variants.csv"
    output:
        "data/{patient}/mutation_counts/count_table/count_table.pickle"
    params:
        missmatch_ratio = config["count_table"]["missmatch_ratio"]
    log:
        "data/{patient}/mutation_counts/logs/make_count_table.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "python scripts/make_count_table.py "
        "-i {input.bam} "
        "-o {output} "
        "-m {input.variants} "
        "-r {params.missmatch_ratio} "
        "2> {log}"


# convert the pickle count table to R dataframe saved as RDS
rule table_to_rds:
    input:
        pickle = "data/{patient}/mutation_counts/count_table/count_table.pickle",
        variants = "raw_data/{patient}/selected_variants.csv",
        barcodes = "raw_data/{patient}/barcodes.tsv",
        reads = "data/{patient}/mutation_counts/reads_cell.txt"
    output:
        "data/{patient}/mutation_counts/count_table/{patient}_count_table.rds"
    params:
        min_umis = config["count_table"]["min_umis_mutant"]
    log:
        "data/{patient}/mutation_counts/logs/table_to_rds.log"
    conda:
        "envs/nuclear_library.yml"
    shell:
        "Rscript scripts/pickle_to_tidytable.R "
        "-i {input.pickle} "
        "-v {input.variants} "
        "-b {input.barcodes} "
        "-r {input.reads} "
        "-t {params.min_umis} "
        "-o {output} "
        "2> {log}"

# make summary report
rule summary_report:
    input:
        variants = "raw_data/{patient}/selected_variants.csv",
        count_table = "data/{patient}/mutation_counts/count_table/{patient}_count_table.rds",
        align_stats = "data/{patient}/mutation_counts/alignment_stats.csv",
        barcodes = "raw_data/{patient}/barcodes.tsv"
    output:
        "results/summary_reports/{patient}/{patient}_report.html"
    log:
        "results/logs/{patient}/summary_report.log"
    conda:
        "envs/r_processing.yml"
    script:
        "scripts/summary_report.Rmd"
