## rules to process mitoTAP data

### input, output and shell paths are all relative to the project directory ###

configfile: "config.yml"

rule all:
    input:
        expand("data/{patient}/align_reads/gene_tagged_aligned.bam",
                    patient = config["patient_ids"]),
        expand("data/{patient}/mito_counts/reads_cell.txt",
               patient = config["patient_ids"]),
        expand("data/{patient}/mito_counts/count_table/umi_collapsed.bam.bai",
                patient = config["patient_ids"]),
        expand("data/{patient}/mito_counts/count_table/{patient}_sum_experiment.rds",
                patient = config["patient_ids"]),
        expand("data/{patient}/mito_counts/count_table/nreads_umi.txt",
                patient = config["patient_ids"]),
        expand("results/summary_reports/{patient}/{patient}_summary_report.html",
                patient = config["patient_ids"])

# workflow rules -----------------------------------------------------------------------------------

import glob

# this is necessary to avoid ambiguous rules
wildcard_constraints:
    patient_novaseq='|'.join(config["patients_novaseq"]),
    patient='|'.join(config["patient_ids"])

# merge fastqs from novaseq runs
rule merge_fastqs:
  input:
    fastqs_read1 = lambda wildcards: sorted(glob.glob("raw_data/{patient_novaseq}/*R1*".format(patient_novaseq = wildcards.patient_novaseq))),
    fastqs_read2 = lambda wildcards: sorted(glob.glob("raw_data/{patient_novaseq}/*R2*".format(patient_novaseq = wildcards.patient_novaseq)))
  output:
    merged_read1 = temp("raw_data/{patient_novaseq}/merged_read1.fastq.gz"),
    merged_read2 = temp("raw_data/{patient_novaseq}/merged_read2.fastq.gz")
  # params:
  #   directory = "raw_data/{patient_novaseq}/unmerged_fastqs"
  log:
    "raw_data/{patient_novaseq}/logs/merge_fastqs.log"
  shell:
    "cat {input.fastqs_read1} > {output.merged_read1} ; "
    "cat {input.fastqs_read2} > {output.merged_read2} "
    "2> {log}"

# some nextseq fastqs need to be sorted...
rule sort_fastqs:
    input:
        fastq1 = "raw_data/{patient}/merged_read1.fastq.gz",
        fastq2 = "raw_data/{patient}/merged_read2.fastq.gz"
    output:
        sorted_read1 = temp("raw_data/{patient}/sorted_read1.fastq.gz"),
        sorted_read2 = temp("raw_data/{patient}/sorted_read2.fastq.gz")
        #sorted_read1 = temp("raw_data/{patient}/read1.fastq.paired.fq.gz"),
        #sorted_read2 = temp("raw_data/{patient}/read2.fastq.paired.fq.gz")
    params:
        gz_read1 = "raw_data/{patient}/read1.fastq",
        gz_read2 = "raw_data/{patient}/read2.fastq",
        uncomp_read1 = "raw_data/{patient}/read1.fastq.paired.fq",
        uncomp_read2 = "raw_data/{patient}/read2.fastq.paired.fq"
    log:
        "raw_data/{patient}/logs/sort_fastqs.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "gunzip -c {input.fastq1} > {params.gz_read1} ; "
        "gunzip -c {input.fastq2} > {params.gz_read2} ; "
        "fastq_pair -t 500000000 {params.gz_read1} {params.gz_read2} ; "
        "gzip {params.uncomp_read1} ; "
        "gzip {params.uncomp_read2} ; "
        "rm {params.gz_read1} {params.gz_read2} ; "
        "rm raw_data/{wildcards.patient}/*single* "
        "2> {log}"


# convert fastq input files into one unmapped bam file for novaseq libraries
rule fastq_to_bam:
  input:
    fastq1 = "raw_data/{patient}/sorted_read1.fastq.gz",
    fastq2 = "raw_data/{patient}/sorted_read2.fastq.gz"
    #fastq1 = "raw_data/{patient}/read1.fastq.paired.fq.gz",
    #fastq2 = "raw_data/{patient}/read2.fastq.paired.fq.gz"
  output:
    temp("data/{patient}/align_reads/unmapped.bam")
  log:
    "data/{patient}/align_reads/logs/fastq_to_bam.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "picard FastqToSam "
    "FASTQ={input.fastq1} "
    "FASTQ2={input.fastq2} "
    "OUTPUT={output} "
    "SAMPLE_NAME={wildcards.patient} "
    "SORT_ORDER=queryname "
    "TMP_DIR=./tmp "
    "2> {log}"

# tag genome reads with CELL barcodes
rule tag_cell_barcodes:
  input:
    "data/{patient}/align_reads/unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/cell_tagged_unmapped.bam"),
    summary = "data/{patient}/align_reads/cell_tags_summary.txt"
  params:
    base_range = config["bc_structure"]["10x_v3"][0],
    base_qual = config["tag_cell_barcodes"]["base_quality"],
    bases_below_qual = config["tag_cell_barcodes"]["num_bases_below_quality"]
  log:
    "data/{patient}/align_reads/logs/tag_cell_barcodes.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "TagBamWithReadSequenceExtended "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "SUMMARY={output.summary} "
    "BASE_QUALITY={params.base_qual} "
    "NUM_BASES_BELOW_QUALITY={params.bases_below_qual} "
    "BASE_RANGE={params.base_range} "
    "BARCODED_READ=1 "
    "DISCARD_READ=false "
    "TAG_NAME=CB "
    "2> {log}"

# tag genome reads with MOLECULE barcodes
rule tag_molecule_barcodes:
  input:
    "data/{patient}/align_reads/cell_tagged_unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/mol_tagged_unmapped.bam"),
    summary = "data/{patient}/align_reads/mol_tags_summary.txt"
  params:
    base_range = config["bc_structure"]["10x_v3"][1],
    base_qual = config["tag_cell_barcodes"]["base_quality"],
    bases_below_qual = config["tag_cell_barcodes"]["num_bases_below_quality"]
  log:
    "data/{patient}/align_reads/logs/tag_molecule_barcodes.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "TagBamWithReadSequenceExtended "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "SUMMARY={output.summary} "
    "BASE_QUALITY={params.base_qual} "
    "NUM_BASES_BELOW_QUALITY={params.bases_below_qual} "
    "BASE_RANGE={params.base_range} "
    "BARCODED_READ=1 "
    "DISCARD_READ=true "
    "TAG_NAME=UB "
    "2> {log}"

# filter reads marked as 'rejected' by TagBamWithReadSequenceExtended
rule filter_bam:
  input:
    "data/{patient}/align_reads/mol_tagged_unmapped.bam"
  output:
    temp("data/{patient}/align_reads/filt_unmapped.bam")
  log:
    "data/{patient}/align_reads/logs/filter_bam.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "FilterBam "
    "INPUT={input} "
    "OUTPUT={output} "
    "TAG_REJECT=XQ "
    "2> {log}"

# trim SMART adapter sequences from 5'
rule trim_starting_sequence:
  input:
    "data/{patient}/align_reads/filt_unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/adapter_trimmed_unmapped.bam"),
    summary = "data/{patient}/align_reads/adapter_trimming_report.txt"
  params:
    adapter_sequence = config["trim_starting_sequence"]["adapter_sequence"],
    mismatches = config["trim_starting_sequence"]["mismatches"],
    num_bases = config["trim_starting_sequence"]["num_bases"]
  log:
    "data/{patient}/align_reads/logs/trim_starting_sequence.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "TrimStartingSequence "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "OUTPUT_SUMMARY={output.summary} "
    "SEQUENCE={params.adapter_sequence} "
    "MISMATCHES={params.mismatches} "
    "NUM_BASES={params.num_bases} "
    "2> {log}"

# trim polyA sequences from 3'
rule trim_polyA:
  input:
    "data/{patient}/align_reads/adapter_trimmed_unmapped.bam"
  output:
    bam = temp("data/{patient}/align_reads/polyA_trimmed_unmapped.bam"),
    summary = "data/{patient}/align_reads/polyA_trimming_report.txt"
  params:
    mismatches = config["trim_polyA"]["mismatches"],
    num_bases = config["trim_polyA"]["num_bases"]
  log:
    "data/{patient}/align_reads/logs/trim_polyA.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "PolyATrimmer "
    "INPUT={input} "
    "OUTPUT={output.bam} "
    "OUTPUT_SUMMARY={output.summary} "
    "MISMATCHES={params.mismatches} "
    "NUM_BASES={params.num_bases} "
    "2> {log}"

# convert to fastq for STAR read aligner
rule sam_to_fastq:
  input:
    "data/{patient}/align_reads/polyA_trimmed_unmapped.bam"
  output:
    temp("data/{patient}/align_reads/polyA_trimmed_unmapped.fastq.gz")
  log:
    "data/{patient}/align_reads/logs/sam_to_fastq.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "picard SamToFastq "
    "INPUT={input} "
    "FASTQ={output} "
    "TMP_DIR=./tmp "
    "2> {log}"

#align reads using STAR
rule star_align:
  input:
    fastq = "data/{patient}/align_reads/polyA_trimmed_unmapped.fastq.gz"
  output:
    bam = temp("data/{patient}/align_reads/star.Aligned.out.bam"),
    final_log = "data/{patient}/align_reads/star.Log.final.out"
  params:
    outprefix = "data/{patient}/align_reads/star.",
    threads = config["star_align"]["threads_comp_bam"],
    genome_dir = config["genome_references"]["genome_dir"]
  log:
    "data/{patient}/align_reads/logs/align.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "STAR "
    "--runThreadN {params.threads} "
    "--genomeDir {params.genome_dir} "
    "--readFilesIn {input.fastq} "
    "--outFileNamePrefix {params.outprefix} "
    "--readFilesCommand zcat "
    "--limitOutSJcollapsed 5000000 "
    "--outSAMtype BAM Unsorted ; "
    # move STAR "progress" logs into log directory
    "mv data/{wildcards.patient}/align_reads/star.Log.progress.out "
    "data/{wildcards.patient}/align_reads/logs ; "
    "mv data/{wildcards.patient}/align_reads/star.Log.out "
    "data/{wildcards.patient}/align_reads/logs ; "
    "mv data/{wildcards.patient}/align_reads/star.SJ.out.tab "
    "data/{wildcards.patient}/align_reads/logs "

# sort aligned reads
rule sort_aligned:
  input:
    "data/{patient}/align_reads/star.Aligned.out.bam"
  output:
    temp("data/{patient}/align_reads/star.Aligned.sorted.bam")
  params:
    tmp_dir = "./tmp"
  log: "data/{patient}/align_reads/logs/sort_aligned.log"
  conda: "../snakemake/envs/mito_mutations.yml"
  shell:
    "picard SortSam "
    "INPUT={input} "
    "OUTPUT={output} "
    "SORT_ORDER=queryname "
    "TMP_DIR=./tmp "
    "2> {log}"


# merge aligned and unaligned reads to add tags to aligned reads. this also removes secondary
# alignments!
rule merge_bam:
  input:
    aligned = "data/{patient}/align_reads/star.Aligned.sorted.bam",
    unaligned = "data/{patient}/align_reads/polyA_trimmed_unmapped.bam",
    reference = config["genome_references"]["fasta"],
    dict = config["genome_references"]["dict"]
  output:
    temp("data/{patient}/align_reads/merged_aligned.bam")
  log:
    "data/{patient}/align_reads/logs/merge_bam.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "picard MergeBamAlignment "
    "ALIGNED_BAM={input.aligned} "
    "UNMAPPED_BAM={input.unaligned} "
    "REFERENCE_SEQUENCE={input.reference} "
    "OUTPUT={output} "
    "INCLUDE_SECONDARY_ALIGNMENTS=false "
    "PAIRED_RUN=false "
    "TMP_DIR=./tmp "
    "2> {log}"


# index BAM file in order to change barcode barcode format
rule index_bam:
    input:
        "data/{patient}/align_reads/merged_aligned.bam"
    output:
        temp("data/{patient}/align_reads/merged_aligned.bam.bai")
    log:
        "data/{patient}/align_reads/logs/index_bam.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "samtools index -b {input} "
        "2> {log}"


# decompress barcodes from output of cellranger
rule decompress_barcodes:
    input:
        "../gene_expression/data/{patient}/outs/filtered_feature_bc_matrix/barcodes.tsv.gz"
    output:
        "data/{patient}/mito_counts/barcodes.tsv"
    params:
        directory = "data/{patient}/mito_counts",
        gzip_file = "data/{patient}/mito_counts/barcodes.tsv.gz"
    log:
        "data/{patient}/mito_counts/logs/decompress_barcodes.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "cp {input} {params.directory} ; "
        "gunzip -c {params.gzip_file} > {output} ; "
        "rm {params.gzip_file} "
        "2> {log}"


# change cell barcodes to cellranger format (adding -1) and filter reads not coming
# from valid cell barcodes (list output by cellranger)
rule reformat_cellbarcode:
    input:
        bam = "data/{patient}/align_reads/merged_aligned.bam",
        bai = "data/{patient}/align_reads/merged_aligned.bam.bai",
        barcodes = "data/{patient}/mito_counts/barcodes.tsv"
    output:
        bam = temp("data/{patient}/align_reads/merged_aligned_tagged.bam")
    log:
        "data/{patient}/align_reads/logs/reformat_barcodes.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "python scripts/reformat_barcode.py "
        "-i {input.bam} "
        "-b {input.barcodes} "
        "-o {output.bam} "
        "2> {log}"


# tag reads with gene exons
rule tag_with_gene_exon:
  input:
    bam = "data/{patient}/align_reads/merged_aligned_tagged.bam",
    annot = config["genome_references"]["refFlat"] # it might need a refFlat file
  output:
    bam = "data/{patient}/align_reads/gene_tagged_aligned.bam"
  log:
    "data/{patient}/align_reads/logs/tag_with_gene_exon.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "TagReadWithGeneFunction "
    "INPUT={input.bam} "
    "OUTPUT={output.bam} "
    "ANNOTATIONS_FILE={input.annot} "
    "CREATE_INDEX=true "
    "2> {log}"


# calculate reads per cell barcode
rule reads_per_cell:
  input:
    "data/{patient}/align_reads/gene_tagged_aligned.bam"
  output:
    "data/{patient}/align_reads/reads_per_cell_barcode.txt"
  params:
    read_quality = config["reads_per_cell"]["read_quality"]
  log:
    "data/{patient}/align_reads/logs/reads_per_cell.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "BamTagHistogram "
    "INPUT={input} "
    "OUTPUT={output} "
    "TAG=CB "
    "READ_MQ={params.read_quality} "
    "2> {log}"


# get total reads per cell
rule get_reads_cell:
    input:
        bam = "data/{patient}/align_reads/gene_tagged_aligned.bam"
    output:
        reads_cell = "data/{patient}/mito_counts/reads_cell.txt"
    log:
        "data/{patient}/mito_counts/logs/get_reads_cell.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "samtools view {input.bam} | awk '{{print $12}}' | sed 's/CB\:Z\://' | sort | uniq -c | "
        "awk '{{if($1 > 100) {{print}} }}' > {output.reads_cell} "
        "2> {log}"

# rule to get present barcodes (it will avoid creating empty single-cell BAM files)
rule get_barcodes:
    input:
        "data/{patient}/align_reads/gene_tagged_aligned.bam"
    output:
        temp("data/{patient}/align_reads/present_barcodes.txt")
    log:
        "data/{patient}/align_reads/logs/get_present_barcodes.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "samtools view {input} | awk '{{print $12}}' | sed 's/CB\:Z\://' | sort | uniq > {output} "
        "2> {log}"

# split barcoded BAM file into single-cell files using CB barcode
# since we don't know in advance the number of files I defined as checkpoint so that the DAG gets reevaluated here
# and in the following rulesls
checkpoint split_bam:
  input:
    bam = "data/{patient}/align_reads/gene_tagged_aligned.bam",
    barcodes = "data/{patient}/mito_counts/barcodes.tsv",
    present_barcodes = "data/{patient}/align_reads/present_barcodes.txt"
  output:
    directory("data/{patient}/mito_counts/count_table/temp_bams/split_bams")
  params:
    cores = config["read_deduplication"]["cores_split_files"],
    max_files = config["read_deduplication"]["max_open_files"]
  log:
    "data/{patient}/mito_counts/logs/split_bam.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "python scripts/split_bam_single_files.py "
    "-i {input.bam} "
    "-o {output} "
    "-b {input.barcodes} "
    "-p {input.present_barcodes} "
    "-c {params.cores} "
    "-m {params.max_files} "
    "2> {log}"

# rule to make consensus reads
rule consensus_read:
  input:
    "data/{patient}/mito_counts/count_table/temp_bams/split_bams/{barcode}.bam"
  output:
    "data/{patient}/mito_counts/count_table/temp_bams/processed/{barcode}.bam"
  log:
    "data/{patient}/mito_counts/logs/make_consensus/{barcode}.log"
  params:
    errors = config["read_deduplication"]["umi_errors"],
    min_reads = config["read_deduplication"]["min_reads_umi"]
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "python scripts/make_consensus_read.py "
    "-i {input} "
    "-o {output} "
    "-e {params.errors} "
    "-r {params.min_reads} "
    "2> {log}"


# make function to aggregate input after checkpoint
def aggregate_bams(wildcards):

    checkpoint_output = checkpoints.split_bam.get(patient = wildcards.patient).output[0]

    return expand("data/{patient}/mito_counts/count_table/temp_bams/processed/{barcode}.bam",
                  patient = wildcards.patient,
                  barcode = glob_wildcards(os.path.join(checkpoint_output, "{barcode}.bam")).barcode)

# merge single-cell BAM files with unique read names to a single file
# it takes ~30' to run, I think because gathering all input files takes a while in Snakemake
rule combine_bams:
  input:
    aggregate_bams
  output:
    temp("data/{patient}/mito_counts/count_table/unmapped_tagged.bam")
  params:
    indir = "data/{patient}/mito_counts/count_table/temp_bams/processed/",
    outdir = "data/{patient}/mito_counts/count_table/",
    max_files = config["read_deduplication"]["max_open_files"],
    cores = config["read_deduplication"]["cores_merge_bam"]
  log:
    "data/{patient}/mito_counts/logs/combine_single_bams.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "python scripts/merge_single_bams.py "
    "-i {params.indir} "
    "-o {output} "
    "-d {params.outdir} "
    "-m {params.max_files} "
    "-c {params.cores} "
    "2> {log}"

# sort bam by queryname
rule sort_bam_query:
 input:
   "data/{patient}/mito_counts/count_table/unmapped_tagged.bam"
 output:
   temp("data/{patient}/mito_counts/count_table/unmapped_sorted.bam")
 log:
   "data/{patient}/mito_counts/logs/sort_merged_bam.log"
 conda:
   "../snakemake/envs/mito_mutations.yml"
 shell:
   "picard SortSam "
   "INPUT={input} "
   "OUTPUT={output} "
   "SORT_ORDER=queryname "
   "TMP_DIR=./tmp "
   "2> {log}"


# transform bam to fastqs for alignment
rule dedupl_bam_fastq:
  input:
    "data/{patient}/mito_counts/count_table/unmapped_sorted.bam"
  output:
    temp("data/{patient}/mito_counts/count_table/consensus.fastq.gz")
  log:
    "data/{patient}/mito_counts/logs/bam_to_fastq.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "picard SamToFastq "
    "INPUT={input} "
    "FASTQ={output} "
    "TMP_DIR=./tmp "
    "2> {log}"

# align deduplicated fastq using STAR (takes ~2-3min file)
rule align_deduplicated:
  input:
    "data/{patient}/mito_counts/count_table/consensus.fastq.gz"
  output:
    bam = temp("data/{patient}/mito_counts/count_table/mapped.bam")
  params:
    threads = config["read_deduplication"]["star_threads"],
    ref_genome = config["genome_references"]["genome_dir"],
    prefix = "data/{patient}/mito_counts/count_table/star/",
    out_bam = "data/{patient}/mito_counts/count_table/star/*.bam"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "mkdir -p {params.prefix} ; "
    "STAR "
    "--runThreadN {params.threads} "
    "--genomeDir {params.ref_genome} "
    "--readFilesIn {input} "
    "--outFileNamePrefix {params.prefix} "
    "--readFilesCommand zcat "
    "--limitOutSJcollapsed 5000000 "
    "--outSAMtype BAM Unsorted ; "
    # move bam file from star_logs folder to count_bams folder
    "mv {params.out_bam} {output.bam}"

# merge aligned and unaligned reads to add tags to aligned reads. this also removes secondary
# alignments!
rule merge_bams:
  input:
    aligned = "data/{patient}/mito_counts/count_table/mapped.bam",
    unaligned = "data/{patient}/mito_counts/count_table/unmapped_sorted.bam",
    reference = config["genome_references"]["fasta"],
  output:
    "data/{patient}/mito_counts/count_table/umi_collapsed.bam"
  log:
    "data/{patient}/mito_counts/logs/merge_bam.log"
  conda:
    "../snakemake/envs/mito_mutations.yml"
  shell:
    "picard MergeBamAlignment "
    "ALIGNED_BAM={input.aligned} "
    "UNMAPPED_BAM={input.unaligned} "
    "REFERENCE_SEQUENCE={input.reference} "
    "OUTPUT={output} "
    "INCLUDE_SECONDARY_ALIGNMENTS=false "
    "PAIRED_RUN=false "
    "TMP_DIR=./tmp "
    "2> {log}"

# index deduplicated sorted bam (do this in the get_mito_counts script)
rule index_deduplicated_bam:
    input:
        "data/{patient}/mito_counts/count_table/umi_collapsed.bam"
    output:
        "data/{patient}/mito_counts/count_table/umi_collapsed.bam.bai"
    log:
        "data/{patient}/mito_counts/logs/index_merged_bam.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "samtools index -b {input} "
        "2> {log}"

# split BAM file to make count tables
checkpoint split_count_bams:
    input:
        bam = "data/{patient}/mito_counts/count_table/umi_collapsed.bam",
        bai = "data/{patient}/mito_counts/count_table/umi_collapsed.bam.bai",
        barcodes = "data/{patient}/align_reads/present_barcodes.txt",
        cell_ranger_barcodes = "data/{patient}/mito_counts/barcodes.tsv"
    output:
        directory("data/{patient}/mito_counts/count_table/temp_bams/count_bams")
    params:
        cores = config["read_deduplication"]["cores_split_files"],
        max_files = config["read_deduplication"]["max_open_files"]
    log:
        "data/{patient}/mito_counts/logs/split_count_bams.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "python scripts/split_count_bam.py "
        "-i {input.bam} "
        "-o {output} "
        "-b {input.cell_ranger_barcodes} "
        "-p {input.barcodes} "
        "-c {params.cores} "
        "-m {params.max_files} "
        "2> {log}"

# make individual count tables for each cell
rule count_reads:
    input:
        "data/{patient}/mito_counts/count_table/temp_bams/count_bams/{barcode}.bam"
    output:
        "data/{patient}/mito_counts/count_table/temp_bams/count_tables/{barcode}.pickle"
    params:
        missmatch_ratio = config["count_table"]["missmatch_ratio"]
    log:
        "data/{patient}/mito_counts/logs/count_reads/{barcode}.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "python scripts/count_single_bam.py "
        "-i {input} "
        "-o {output} "
        "-r {params.missmatch_ratio} "
        "2> {log}"

# make function to aggregate input after checkpoint
def aggregate_tables(wildcards):

    checkpoint_output = checkpoints.split_count_bams.get(patient = wildcards.patient).output[0]

    return expand("data/{patient}/mito_counts/count_table/temp_bams/count_tables/{barcode}.pickle",
                  patient = wildcards.patient,
                  barcode = glob_wildcards(os.path.join(checkpoint_output, "{barcode}.bam")).barcode)


# convert the pickle count table to R dataframe saved as RDS
rule make_count_table:
    input:
        aggregate_tables
    output:
        count_tables = "data/{patient}/mito_counts/count_table/{patient}_count_table.rds",
        sum_experiment = "data/{patient}/mito_counts/count_table/{patient}_sum_experiment.rds"
    params:
        barcodes = "data/{patient}/mito_counts/barcodes.tsv",
        in_dir = "data/{patient}/mito_counts/count_table/temp_bams/count_tables",
        reads = "data/{patient}/mito_counts/reads_cell.txt",
        cores = config["count_table"]["cores"]
    log:
        "data/{patient}/mito_counts/logs/make_count_table.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "Rscript scripts/pickle_to_RDS.R "
        "-i {params.in_dir} "
        "-b {params.barcodes} "
        "-r {params.reads} "
        "-s {output.sum_experiment} "
        "-o {output.count_tables} "
        "-c {params.cores} "
        "-k FALSE "
        "2> {log}"

# get number of reads/UMI as txt file
rule reads_umi:
    input:
        bam = "data/{patient}/mito_counts/count_table/umi_collapsed.bam",
        bai = "data/{patient}/mito_counts/count_table/umi_collapsed.bam.bai"
    output:
        "data/{patient}/mito_counts/count_table/nreads_umi.txt"
    log:
        "data/{patient}/mito_counts/logs/nreads_umi.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    shell:
        "python scripts/nreads_umi.py "
        "-i {input.bam} "
        "-o {output} "
        "2> {log}"

# make summary report
rule summary_report:
    input:
        count_table = "data/{patient}/mito_counts/count_table/{patient}_sum_experiment.rds",
        align_stats = "data/{patient}/align_reads/star.Log.final.out",
        barcodes = "data/{patient}/mito_counts/barcodes.tsv",
        reads_umi = "data/{patient}/mito_counts/count_table/nreads_umi.txt"
    output:
        "results/summary_reports/{patient}/{patient}_summary_report.html"
    params:
        samples = lambda wildcards: config["samples_report"][wildcards.patient]
    log:
        "results/logs/{patient}/summary_report.log"
    conda:
        "../snakemake/envs/mito_mutations.yml"
    script:
        "scripts/summary_report.Rmd"
